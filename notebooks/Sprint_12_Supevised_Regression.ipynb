{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python385jvsc74a57bd031f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6",
   "display_name": "Python 3.8.5 64-bit"
  },
  "metadata": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# IT Academy - Data Science with Python\n",
    "## Sprint 12: Supervised Regressions\n",
    "### [Github Supervised Regressions](https://github.com/jesussantana/Supervised-Regression)\n",
    "\n",
    "[![forthebadge made-with-python](http://ForTheBadge.com/images/badges/made-with-python.svg)](https://www.python.org/)  \n",
    "[![Made withJupyter](https://img.shields.io/badge/Made%20with-Jupyter-orange?style=for-the-badge&logo=Jupyter)](https://jupyter.org/try)  \n",
    "[![wakatime](https://wakatime.com/badge/github/jesussantana/Supervised-Regression.svg)](https://wakatime.com/badge/github/jesussantana/Supervised-Regression)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"%pip install tabulate\n",
    "%pip install scikit-optimize\n",
    "%pip install fitter\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "new_path = '../scripts/'\n",
    "if new_path not in sys.path:\n",
    "    sys.path.append(new_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "SyntaxError",
     "evalue": "invalid syntax (setting.py, line 82)",
     "traceback": [
      "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
      "  File \u001b[1;32m\"/home/jesus/.local/lib/python3.8/site-packages/IPython/core/interactiveshell.py\"\u001b[0m, line \u001b[1;32m3437\u001b[0m, in \u001b[1;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-2-b64d5ec6b1b3>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0;36m, in \u001b[0;35m<module>\u001b[0;36m\u001b[0m\n\u001b[0;31m    import setting\u001b[0m\n",
      "\u001b[0;36m  File \u001b[0;32m\"../scripts/setting.py\"\u001b[0;36m, line \u001b[0;32m82\u001b[0m\n\u001b[0;31m    %matplotlib inline\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data treatment\n",
    "# ==============================================================================\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import json_normalize\n",
    "from datetime import datetime\n",
    "from tabulate import tabulate\n",
    "\n",
    "# # Graphics\n",
    "# ==============================================================================\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from matplotlib import style\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import geopandas as gpd\n",
    "import cartopy.crs as ccrs\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "from PIL import Image\n",
    "from IPython.display import Image\n",
    "\n",
    "# Preprocessing and modeling\n",
    "# ==============================================================================\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from statsmodels.formula.api import ols\n",
    "\n",
    "from scipy import stats\n",
    "from scipy.stats import ttest_1samp,ttest_ind\n",
    "from scipy.stats import normaltest\n",
    "from scipy.stats import f_oneway\n",
    "from scipy.stats.mstats import gmean,hmean\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.metrics import euclidean_distances\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from skopt import gp_minimize\n",
    "from skopt.space import Real, Integer\n",
    "from skopt.utils import use_named_args\n",
    "from skopt.plots import plot_convergence\n",
    "\n",
    "# Paralllel Processing\n",
    "# ==============================================================================\n",
    "import multiprocessing\n",
    "from joblib import Parallel, delayed\n",
    "import numba\n",
    "from numba import jit\n",
    "\n",
    "# Various\n",
    "# ==============================================================================\n",
    "import time\n",
    "import random as rd\n",
    "from itertools import product\n",
    "from fitter import Fitter, get_common_distributions\n",
    "from device_detector import SoftwareDetector\n",
    "\n",
    "# Pandas configuration\n",
    "# ==============================================================================\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# Matplotlib configuration\n",
    "# ==============================================================================\n",
    "plt.rcParams['image.cmap'] = \"bwr\"\n",
    "#plt.rcParams['figure.dpi'] = \"100\"\n",
    "plt.rcParams['savefig.bbox'] = \"tight\"\n",
    "style.use('ggplot') or plt.style.use('ggplot')\n",
    "%matplotlib inline\n",
    "\n",
    "# Seaborn configuration\n",
    "# ==============================================================================\n",
    "sns.set_theme(style='darkgrid', palette='deep')\n",
    "dims = (20, 16)\n",
    "\n",
    "# Warnings configuration\n",
    "# ==============================================================================\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Folder configuration\n",
    "# ==============================================================================\n",
    "from os import path\n",
    "import sys\n",
    "new_path = '../scripts/'\n",
    "if new_path not in sys.path:\n",
    "    sys.path.append(new_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path folder configuration\n",
    "# ===============================================================================\n",
    "\n",
    "path = \"../data/\"\n",
    "file = \"processed/DelayedFlightsProcessed.csv\"\n",
    "\n",
    "df_raw = pd.read_csv(path + file)"
   ]
  },
  {
   "source": [
    "### Exercise 1: \n",
    "  - Create at least three different regression models to try to best predict DelayedFlights.csv flight delay (ArrDelay)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Exercise 2: \n",
    "  - Compare them based on MSE and R2."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Exercise 3: \n",
    "  - Train them using the different parameters they support"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Exercise 4: \n",
    "  - Compare your performance using the traint / test approach or using all data (internal validation)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_raw.copy()"
   ]
  },
  {
   "source": [
    "## Exploratory analysis"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum().sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df)"
   ]
  },
  {
   "source": [
    "## Distribution of the response variable"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=3, ncols=1, figsize=(6, 6))\n",
    "sns.distplot(\n",
    "    df.ArrTime,\n",
    "    hist    = False,\n",
    "    rug     = True,\n",
    "    color   = \"blue\",\n",
    "    kde_kws = {'shade': True, 'linewidth': 1},\n",
    "    ax      = axes[0]\n",
    ")\n",
    "axes[0].set_title(\"Original layout\", fontsize = 'medium')\n",
    "axes[0].set_xlabel('ArrTime', fontsize='small') \n",
    "axes[0].tick_params(labelsize = 6)\n",
    "\n",
    "sns.distplot(\n",
    "    np.sqrt(df.ArrTime),\n",
    "    hist    = False,\n",
    "    rug     = True,\n",
    "    color   = \"blue\",\n",
    "    kde_kws = {'shade': True, 'linewidth': 1},\n",
    "    ax      = axes[1]\n",
    ")\n",
    "axes[1].set_title(\"Square root transformation\", fontsize = 'medium')\n",
    "axes[1].set_xlabel('sqrt(ArrTime)', fontsize='small') \n",
    "axes[1].tick_params(labelsize = 6)\n",
    "\n",
    "sns.distplot(\n",
    "    np.log(df.ArrTime),\n",
    "    hist    = False,\n",
    "    rug     = True,\n",
    "    color   = \"blue\",\n",
    "    kde_kws = {'shade': True, 'linewidth': 1},\n",
    "    ax      = axes[2]\n",
    ")\n",
    "axes[2].set_title(\"Logarithmic transformation\", fontsize = 'medium')\n",
    "axes[2].set_xlabel('log(ArrTime)', fontsize='small') \n",
    "axes[2].tick_params(labelsize = 6)\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "source": [
    "## Identify which distribution the data best fit "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distributions = ['cauchy', 'chi2', 'expon',  'exponpow', 'gamma',\n",
    "                  'norm', 'powerlaw', 'beta', 'logistic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitter = Fitter(df.ArrTime, distributions=distributions)\n",
    "fitter.fit()\n",
    "fitter.summary(Nbest=10, plot=False)"
   ]
  },
  {
   "source": [
    "## Numerical variables"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select_dtypes(include=['float64', 'int']).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"# Distribution graph for each numerical variable\n",
    "# ==============================================================================\n",
    "# Adjust number of subplots based on the number of columns\n",
    "\n",
    "fig, axes = plt.subplots(nrows=3, ncols=3, figsize=(9, 5))\n",
    "axes = axes.flat\n",
    "columnas_numeric = df.select_dtypes(include=['float64', 'int']).columns\n",
    "columnas_numeric = columnas_numeric.drop('ArrTime')\n",
    "\n",
    "for i, colum in enumerate(columnas_numeric):\n",
    "    sns.histplot(\n",
    "        data    = df,\n",
    "        x       = colum,\n",
    "        stat    = \"count\",\n",
    "        kde     = True,\n",
    "        color   = (list(plt.rcParams['axes.prop_cycle'])*2)[i][\"color\"],\n",
    "        line_kws= {'linewidth': 2},\n",
    "        alpha   = 0.3,\n",
    "        ax      = axes[i]\n",
    "    )\n",
    "    axes[i].set_title(colum, fontsize = 7, fontweight = \"bold\")\n",
    "    axes[i].tick_params(labelsize = 6)\n",
    "    axes[i].set_xlabel(\"\")\n",
    "    \n",
    "    \n",
    "fig.tight_layout()\n",
    "plt.subplots_adjust(top = 0.9)\n",
    "fig.suptitle('Distribution Numerical Variable', fontsize = 10, fontweight = \"bold\")\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"# Distribution graph for each numerical variable\n",
    "# ==============================================================================\n",
    "# Adjust number of subplots based on the number of columns\n",
    "fig, axes = plt.subplots(nrows=3, ncols=3, figsize=(4, 5))\n",
    "axes = axes.flat\n",
    "columnas_numeric = df.select_dtypes(include=['float64', 'int']).columns\n",
    "columnas_numeric = columnas_numeric.drop('ArrTime')\n",
    "\n",
    "for i, colum in enumerate(columnas_numeric):\n",
    "    sns.regplot(\n",
    "        x           = df[colum],\n",
    "        y           = df['ArrTime'],\n",
    "        color       = \"gray\",\n",
    "        marker      = '.',\n",
    "        scatter_kws = {\"alpha\":0.4},\n",
    "        line_kws    = {\"color\":\"r\",\"alpha\":0.7},\n",
    "        ax          = axes[i]\n",
    "    )\n",
    "    axes[i].set_title(f\"ArrTime vs {colum}\", fontsize = 7, fontweight = \"bold\")\n",
    "    #axes[i].ticklabel_format(style='sci', scilimits=(-4,4), axis='both')\n",
    "    axes[i].yaxis.set_major_formatter(ticker.EngFormatter())\n",
    "    axes[i].xaxis.set_major_formatter(ticker.EngFormatter())\n",
    "    axes[i].tick_params(labelsize = 6)\n",
    "    axes[i].set_xlabel(\"\")\n",
    "    axes[i].set_ylabel(\"\")\n",
    "\n",
    "    #if (i-1 >= len(columnas_numeric)-1): break\n",
    "\n",
    "# Se eliminan los axes vacíos\n",
    "for i in [8]:\n",
    "    fig.delaxes(axes[i])\n",
    "    \n",
    "fig.tight_layout()\n",
    "plt.subplots_adjust(top=0.9)\n",
    "fig.suptitle('Correlación con ArrTime', fontsize = 10, fontweight = \"bold\");\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrix\n",
    "X = df.iloc[:, 6].values.reshape((-1, 1))\n",
    "# Vector\n",
    "y = df.iloc[:, 0].values"
   ]
  },
  {
   "source": [
    "- Divide the data set into training set and test set"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of data in train and test\n",
    "# ==============================================================================\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "                                        df.drop('ArrTime', axis = 'columns'),\n",
    "                                        df['ArrTime'],\n",
    "                                        train_size   = 0.8,\n",
    "                                        random_state = 6858,\n",
    "                                        shuffle      = True\n",
    "                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\">>>Train partition\")\n",
    "print(\"-----------------------\")\n",
    "print(y_train.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Test partition\")\n",
    "print(\"-----------------------\")\n",
    "print(y_test.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selección de las variables por típo\n",
    "# ==============================================================================\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import make_column_selector\n",
    "\n",
    "# Se estandarizan las columnas numéricas y se hace one-hot-encoding de las \n",
    "# columnas cualitativas. Para mantener las columnas a las que no se les aplica \n",
    "# ninguna transformación se tiene que indicar remainder='passthrough'.\n",
    "numeric_cols = X_train.select_dtypes(include=['float64', 'int']).columns.to_list()\n",
    "cat_cols = X_train.select_dtypes(include=['object', 'category']).columns.to_list()\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "                   [('scale', StandardScaler(), numeric_cols),\n",
    "                    ('onehot', OneHotEncoder(handle_unknown='ignore'), cat_cols)],\n",
    "                remainder='passthrough')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_prep = preprocessor.fit_transform(X_train)\n",
    "X_test_prep  = preprocessor.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir el output en dataframe y añadir el nombre de las columnas\n",
    "# ==============================================================================\n",
    "encoded_cat = preprocessor.named_transformers_['onehot'].get_feature_names(cat_cols)\n",
    "labels = np.concatenate([numeric_cols, encoded_cat])\n",
    "datos_train_prep = preprocessor.transform(X_train)\n",
    "datos_train_prep = pd.DataFrame(datos_train_prep, columns=labels)\n",
    "datos_train_prep.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Selección de las variables por típo\n",
    "# ==============================================================================\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import make_column_selector\n",
    "\n",
    "numeric_cols = X_train.select_dtypes(include=['float64', 'int']).columns.to_list()\n",
    "cat_cols = X_train.select_dtypes(include=['object', 'category']).columns.to_list()\n",
    "\n",
    "# Transformaciones para las variables numéricas\n",
    "numeric_transformer = Pipeline(\n",
    "                        steps=[\n",
    "                            ('imputer', SimpleImputer(strategy='median')),\n",
    "                            ('scaler', StandardScaler())\n",
    "                        ]\n",
    "                      )\n",
    "\n",
    "\n",
    "# Transformaciones para las variables categóricas\n",
    "categorical_transformer = Pipeline(\n",
    "                            steps=[\n",
    "                                ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "                                ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "                            ]\n",
    "                          )\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "                    transformers=[\n",
    "                        ('numeric', numeric_transformer, numeric_cols),\n",
    "                        ('cat', categorical_transformer, cat_cols)\n",
    "                    ],\n",
    "                    remainder='passthrough'\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_prep = preprocessor.fit_transform(X_train)\n",
    "X_test_prep  = preprocessor.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the output to a dataframe and add the name of the columns\n",
    "# ==============================================================================\n",
    "\n",
    "encoded_cat = preprocessor.named_transformers_['cat']['onehot']\\\n",
    "              .get_feature_names(cat_cols)\n",
    "labels = np.concatenate([numeric_cols, encoded_cat])\n",
    "df_train_prep = preprocessor.transform(X_train)\n",
    "df_train_prep = pd.DataFrame(df_train_prep, columns=labels)\n",
    "df_train_prep.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import set_config\n",
    "set_config(display='diagram')\n",
    "\n",
    "preprocessor"
   ]
  },
  {
   "source": [
    "- Create Simple Linear Regression model with training set"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "- Linear regressions Internal, External"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error,r2_score\n",
    "\n",
    "# Intern\n",
    "regr_INT = linear_model.LinearRegression()\n",
    "regr_INT.fit(X,y)\n",
    "pred_INT = regr_INT.predict(X)\n",
    "print(\"R2 Intern: %.4f\" % r2_score(y, pred_INT))\n",
    "print(\"MSE: %.4f\" % mean_squared_error(y, pred_INT))\n",
    "\n",
    "# Extern\n",
    "regr_EXT = linear_model.LinearRegression()\n",
    "regr_EXT.fit(X_train,y_train)\n",
    "pred_EXT = regr_EXT.predict(X_test)\n",
    "print(\"\\nR2 Extern: %.4f\" %  r2_score(y_test, pred_EXT))\n",
    "print(\"MSE Extern: %.4f\" %  mean_squared_error(y_test, pred_EXT))"
   ]
  },
  {
   "source": [
    "- View training results"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_train, y_train, color = \"red\")\n",
    "plt.plot(X_train, regr_EXT.predict(X_train), color = \"blue\")\n",
    "plt.title(\"Departure Delay vs Arrived Delay (Trainning Set)\")\n",
    "plt.xlabel(\"Departure Delay\")\n",
    "plt.ylabel(\"Arrived Delay\")\n",
    "plt.show()"
   ]
  },
  {
   "source": [
    "## Multiple Linear Regression"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrix\n",
    "X = df.iloc[:, :-1].values\n",
    "# Vector\n",
    "y = df.iloc[:, 0].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regression = LinearRegression()\n",
    "regression.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = regression.predict(X_test)"
   ]
  },
  {
   "source": [
    "- Build the optimal RLM model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_opt = X[:, [1, 2, 3, 4, 5, 6]]\n",
    "regression_OLS = sm.OLS(endog = y, exog = X_opt.tolist()).fit()\n",
    "regression_OLS.summary()\n",
    "\n",
    "X_opt = X[:, [1, 2, 3, 4, 5]]\n",
    "regression_OLS = sm.OLS(endog = y, exog = X_opt.tolist()).fit()\n",
    "regression_OLS.summary()\n",
    "\n",
    "X_opt = X[:, [1, 2, 3, 4]]\n",
    "regression_OLS = sm.OLS(endog = y, exog = X_opt.tolist()).fit()\n",
    "regression_OLS.summary()\n",
    "\n",
    "X_opt = X[:, [1, 2, 3]]\n",
    "regression_OLS = sm.OLS(endog = y, exog = X_opt.tolist()).fit()\n",
    "regression_OLS.summary()\n",
    "\n",
    "X_opt = X[:, [1, 2]]\n",
    "regression_OLS = sm.OLS(endog = y, exog = X_opt.tolist()).fit()\n",
    "regression_OLS.summary()"
   ]
  },
  {
   "source": [
    "- Build the optimal RLM model using Automatic Backward Elimination"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backwardElimination(x, sl):    \n",
    "    numVars = len(x[0])    \n",
    "    for i in range(0, numVars):        \n",
    "        regressor_OLS = sm.OLS(y, x.tolist()).fit()        \n",
    "        maxVar = max(regressor_OLS.pvalues).astype(float)        \n",
    "        if maxVar > sl:            \n",
    "            for j in range(0, numVars - i):                \n",
    "                if (regressor_OLS.pvalues[j].astype(float) == maxVar):                    \n",
    "                    x = np.delete(x, j, 1)    \n",
    "        \n",
    "    return x, regressor_OLS.summary()\n",
    " \n",
    "SL = 0.05\n",
    "X_opt = X[:, [1, 2, 3, 4, 5, 6]]\n",
    "X_Modeled, summary = backwardElimination(X_opt, SL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_Modeled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary"
   ]
  },
  {
   "source": [
    "## KFold"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sample(frac=1).head(5000)\n",
    "\n",
    "df = df.reset_index() # Importante resetear index al hacer seleccion aleatoria\n",
    "\n",
    "X = df[[\"ArrTime\", \"Distance\", \"DepDelay\"]]\n",
    "y = df[\"ArrDelay\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replicar el proceso anterior multiples veces \n",
    "from sklearn.model_selection import KFold \n",
    "\n",
    "kf = KFold(n_splits= 10, shuffle = True)  # particiones / mezcla de datos\n",
    "\n",
    "kf.get_n_splits(X) # particiones concretas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ajustar un modelo regresion lineal\n",
    "#regr = linear_model.LinearRegression()\n",
    "regr_INT = linear_model.LinearRegression()\n",
    "regr_EXT = linear_model.LinearRegression()\n",
    "resultados_Interno = []\n",
    "resultados_Externo = []\n",
    "\n",
    "for train_index, test_index in kf.split(X):  # indices\n",
    "    X_train, X_test = X.loc[train_index,], X.loc[test_index] # objetos validacion externa\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \"\"\"regr.fit(X_train, y_train)  #ajustar el modelo\n",
    "    predicciones = regr.predict(X_test)  # prediciones test\n",
    "    print(\"R2: %.4f\" %  r2_score(y_test, predicciones))  # evaluar con test\n",
    "    print(\"MSE: %.4f\" %  mean_squared_error(y_test, predicciones))\"\"\"\n",
    "\n",
    "    # Extern\n",
    "\n",
    "    regr_EXT.fit(X_train,y_train)\n",
    "    pred_EXT = regr_EXT.predict(X_test)\n",
    "    print(\"\\nR2 Extern: %.4f\" %  r2_score(y_test, pred_EXT))\n",
    "    print(\"MSE Extern: %.4f\" %  mean_squared_error(y_test, pred_EXT))\n",
    "\n",
    "    resultados_Externo.append(r2_score(y_test, pred_EXT))\n",
    "\n",
    "\n",
    "\"\"\"for train_index, test_index in kf.split(X):  # indices\n",
    "    X_train, X_test = X.loc[train_index,], X.loc[test_index] # objetos validacion externa\n",
    "    y_train, y_test = y[train_index], y[test_index]\"\"\"\n",
    "\n",
    "   \n",
    "\n",
    "\n",
    "\n",
    "print(\"\\nR2 medio: %.4f\"% np.mean(resultados_Externo))\n",
    "\n",
    "    # Intern\n",
    "\n",
    "regr_INT.fit(X,y)\n",
    "pred_INT = regr_INT.predict(X)\n",
    "print(\"\\nR2 Intern: %.4f\" % r2_score(y, pred_INT))\n",
    "print(\"MSE: %.4f\" % mean_squared_error(y, pred_INT))\n",
    "\n",
    "\"\"\"resultados_Interno.append(r2_score(y_test, pred_INT)) \n",
    "\n",
    "print(\"R2 medio: %.4f\"% np.mean(resultados_Interno))\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# como de dependiente es la evaluacion del modelo en relacion del conjunto train,test utilizado"
   ]
  },
  {
   "source": [
    "## Polynomial Regression"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample = df.sample(n=25, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrix\n",
    "X = df_sample.iloc[:, 7:8].values\n",
    "\n",
    "# Vector\n",
    "y = df_sample.iloc[:, 0:1].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape\n",
    "y.shape"
   ]
  },
  {
   "source": [
    "- Fit Polynomial regression with the dataset"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "poly_reg = PolynomialFeatures(degree = 2)\n",
    "X_poly = poly_reg.fit_transform(X)\n",
    "\n",
    "lin_reg_2 = LinearRegression()\n",
    "lin_reg_2.fit(X_poly, y)"
   ]
  },
  {
   "source": [
    "## Visualization of the results of the Polynomial Model vs Linear Regression"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X, y, color = \"red\")\n",
    "plt.plot(X, lin_reg.predict(X), color = \"blue\")\n",
    "plt.title(\"Linear Regression Model\")\n",
    "plt.xlabel(\"Deep Delay\")\n",
    "plt.ylabel(\"Arrived Delay\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_grid = np.arange(min(X), max(X), 0.1)\n",
    "X_grid = X_grid.reshape(len(X_grid), 1)\n",
    "\n",
    "plt.scatter(X, y, color = \"red\")\n",
    "plt.plot(X_grid, lin_reg_2.predict(poly_reg.fit_transform(X_grid)), color = \"blue\")\n",
    "plt.title(\"Polynomial Regression Model\")\n",
    "plt.xlabel(\"Deep Delay\")\n",
    "plt.ylabel(\"Arrived Delay\")\n",
    "plt.show()"
   ]
  }
 ]
}